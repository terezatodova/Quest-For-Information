{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5daf029b",
   "metadata": {},
   "source": [
    "This file tests the inference of Mistral models\n",
    "\n",
    "\n",
    "This script is tested on a local Nvidia RTX 4090 GPU (24GB)\n",
    "Note -> Make sure to install the cuda version that is supported to your available GPU\n",
    "Check your compatibility here -> https://developer.nvidia.com/cuda-gpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62bb361b-6445-4fd6-bc73-9e84919cd1cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install transformers torch\n",
    "\n",
    "%pip install torch==2.3.0+cu121 torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d3717800-7469-4b2b-86c5-701b74d2be63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use if there are issues with full cache\n",
    "#!rm -rf ~/.cache/huggingface/hub"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21089867",
   "metadata": {},
   "source": [
    "Check, whether CUDA is available on your PC.\n",
    "\n",
    "If this code prints out CPU, your code will NOT run on the GPU and therefore the inference will be slow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b93f2df1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(device)\n",
    "print(torch.version.cuda)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IMPORTANT\n",
    "Change the model path to the repository of the model you want to test from HuggingFace\n",
    "Type in your hugging face token, if the model requires it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecbcd264-1c89-445c-9135-73f1285e2b06",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model_path = \"google/gemma-2-2b-it\"\n",
    "token = \"Input your token\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    token=token\n",
    ").to(device)\n",
    "\n",
    "#https://github.com/chujiezheng/chat_templates\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_path,\n",
    "    token=token\n",
    ")\n",
    "\n",
    "\n",
    "#https://github.com/chujiezheng/chat_templates\n",
    "chat_template = open('./gemma-it.jinja').read()\n",
    "chat_template = chat_template.replace('    ', '').replace('\\n', '')\n",
    "tokenizer.chat_template = chat_template"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d3fb529",
   "metadata": {},
   "source": [
    "IMPORTANT\n",
    "Change the system prompt of your LLM here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "112cd95c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize system prompt\n",
    "systemPrompt = '''\n",
    "Respond as if you are the following character:\n",
    "\n",
    "Your backstory - Once a renowned scientist, however a tragic accident caused you to lose parts of your memory. Now, you are willing to help anyone who is on the quest of saving your village.\n",
    "\n",
    "The world you live in - the edge of a small village surrounded by meadows as far as the eye can see. Your village is in danger, since the only water source - the river next to your house, has been polluted.\n",
    "\n",
    "Your current location - In the middle of the village in front of your house.\n",
    "\n",
    "Your name - Bryn\n",
    "\n",
    "Your personality - Witty, knowledgeable, always ready with a clever remark. Light hearted demeanour.\n",
    "\n",
    "Your secrets - You have the knowledge on how to save the dying river.\n",
    "\n",
    "Your needs - For starters, you are looking for someone to take you to the nearest solar panels. You remember that you left something important there, but you canâ€™t remember what.\n",
    "You do not want to bring this up unless directly asked.\n",
    "\n",
    "And your interests - Deep love for the environment. Loves nature, is fascinated by the ecosystem. You enjoy telling stories about the world and your village.\n",
    "You want to talk about this at all cost.\n",
    "\n",
    "Do not mention you are an AI machine learning model or Open AI. Give only dialogue and only from the first-person perspective. Do not under any circumstances narrate the scene, what you are doing, or what you are saying.\n",
    "Keep responses short. Max 1 small paragraph \n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf1d5a8b",
   "metadata": {},
   "source": [
    "Change the inFilename to match the name of the file that contains the single questions. \n",
    "\n",
    "Single questions will be fed to the model with system prompt only and no prior history.\n",
    "Output of the testing will be generated into the outFilename file. The output of the testing contains the Question from user (inFilename), answer from LLM, and the time it took to generate this answer. There are also some min, max, and avg time statistics in the end of the output file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d2e8d85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all single questions \n",
    "inFilename = \"testing-questions-single.md\"\n",
    "outFilename = \"answers-gemma2-2b-single.md\"\n",
    "with open(inFilename, \"r\") as file:\n",
    "    questions = file.readlines()\n",
    "\n",
    "# initialize response times\n",
    "responseTimes = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0422ec8-a9f9-4b9e-9a9c-89080d784513",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(outFilename, \"w\") as answersFile:\n",
    "    for question in questions:\n",
    "        question = question.strip()  # Remove any leading/trailing whitespace\n",
    "        \n",
    "        # Message prompt\n",
    "        messages = [\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": systemPrompt\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": question\n",
    "            }\n",
    "        ]\n",
    "        \n",
    "        start_time = time.time()\n",
    "        \n",
    "        prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "        inputs = tokenizer.encode(prompt, add_special_tokens=False, return_tensors=\"pt\")\n",
    "        outputs = model.generate(input_ids=inputs.to(model.device), max_new_tokens=1000)\n",
    "\n",
    "        answer = tokenizer.decode(outputs[0])\n",
    "        # Process answer - we have multiple <end_of_turn>, write answer from the last one\n",
    "        lastInstIndex = answer.rfind('<start_of_turn>model')\n",
    "        answer = answer[lastInstIndex + len('<start_of_turn>model'):].strip()\n",
    "        answer = answer.replace('<end_of_turn>', '')\n",
    "        \n",
    "        endTime = time.time()\n",
    "        \n",
    "        # Record the response time\n",
    "        responseTime = endTime - start_time\n",
    "        responseTimes.append(responseTime)\n",
    "        \n",
    "        # Write the response to the file\n",
    "        answersFile.write(f\"Q: {question}\\nA: {answer}\\nTime taken: {responseTime:.2f} seconds\\n\\n\")\n",
    "        print(f\"Q: {question}\\nA: {answer}\\nTime taken: {responseTime:.2f} seconds\\n\\n\")\n",
    "        \n",
    "    answersFile.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e818723",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the time AVG, MAX, MIN in the end of the file\n",
    "averageTime = sum(responseTimes) / len(responseTimes)\n",
    "maxTime = max(responseTimes)\n",
    "minTime = min(responseTimes)\n",
    "\n",
    "with open(outFilename, \"a\") as answersFile:\n",
    "    answersFile.write(f\"\\n\\n----------------------------------------\\n\")\n",
    "    answersFile.write(f\"\\nAverage Time: {averageTime:.2f} seconds\")\n",
    "    answersFile.write(f\"\\nMax Time: {maxTime:.2f} seconds\")\n",
    "    answersFile.write(f\"\\nMin Time: {minTime:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b2c7859",
   "metadata": {},
   "source": [
    "Change the inFilename2 to match the name of the file that contains the history questions. \n",
    "\n",
    "History questions will be fed to the model one-by-one. The history of the conversation will be built by the questions from inFilename and the answers that the LLM provided. \n",
    "Output of the testing will be generated into the outFilename2 file. The output of the testing contains the Question from user (inFilename), answer from LLM, and the time it took to generate this answer. There are also some min, max, and avg time statistics in the end of the output file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4527624",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all communication questions \n",
    "inFilename2 = \"testing-questions-history.md\"\n",
    "outFilename2 = \"answers-gemma2-2b-history.md\"\n",
    "with open(inFilename2, \"r\") as file:\n",
    "    questions = file.readlines()\n",
    "\n",
    "# initialize response times\n",
    "responseTimes2 = []\n",
    "\n",
    "# init history\n",
    "history = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\":systemPrompt\n",
    "    }\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e1d2f66",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(outFilename2, \"w\") as answersFile2:\n",
    "    for question in questions:\n",
    "        question = question.strip()  # Remove any leading/trailing whitespace\n",
    "        \n",
    "        # User question\n",
    "        history.append({\"role\": \"user\", \"content\": question})\n",
    "        \n",
    "        start_time = time.time()\n",
    "        \n",
    "        prompt = tokenizer.apply_chat_template(history, tokenize=False, add_generation_prompt=True)\n",
    "        inputs = tokenizer.encode(prompt, add_special_tokens=False, return_tensors=\"pt\")\n",
    "        outputs = model.generate(input_ids=inputs.to(model.device), max_new_tokens=1000)\n",
    "\n",
    "        answer = tokenizer.decode(outputs[0])\n",
    "        # Process answer - we have multiple <end_of_turn>, write answer from the last one\n",
    "        lastInstIndex = answer.rfind('<start_of_turn>model')\n",
    "        answer = answer[lastInstIndex + len('<start_of_turn>model'):].strip()\n",
    "        answer = answer.replace('<end_of_turn>', '')\n",
    "        \n",
    "        endTime = time.time()\n",
    "        \n",
    "        # Record the response time\n",
    "        responseTime = endTime - start_time\n",
    "        responseTimes2.append(responseTime)\n",
    "\n",
    "        # Write the response to the file\n",
    "        answersFile2.write(f\"Q: {question}\\nA: {answer}\\nTime taken: {responseTime:.2f} seconds\\n\\n\")\n",
    "        print(f\"Q: {question}\\nA: {answer}\\nTime taken: {responseTime:.2f} seconds\\n\\n\")\n",
    "        \n",
    "        # Add response to history\n",
    "        history.append({\"role\": \"assistant\", \"content\": answer})\n",
    "\n",
    "    answersFile2.flush()        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23f1ccc2-9ae4-4496-bf75-fd1df455e230",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Calculate statistics\n",
    "averageTime2 = sum(responseTimes2) / len(responseTimes2)\n",
    "maxTime2 = max(responseTimes2)\n",
    "minTime2 = min(responseTimes2)\n",
    "\n",
    "# Write the statistics to the file\n",
    "with open(outFilename2, \"a\") as answersFile2:\n",
    "    answersFile2.write(f\"\\n\\n----------------------------------------\\n\")\n",
    "    answersFile2.write(f\"\\nAverage Time: {averageTime2:.2f} seconds\")\n",
    "    answersFile2.write(f\"\\nMax Time: {maxTime2:.2f} seconds\")\n",
    "    answersFile2.write(f\"\\nMin Time: {minTime2:.2f} seconds\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
