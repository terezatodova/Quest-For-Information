{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3d88c8c3",
   "metadata": {},
   "source": [
    "This file tests the inference of LLama models with an extended system prompt\n",
    "\n",
    "\n",
    "This script is tested on a local Nvidia RTX 4090 GPU (24GB)\n",
    "Note -> Make sure to install the cuda version that is supported to your available GPU\n",
    "Check your compatibility here -> https://developer.nvidia.com/cuda-gpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12986431-1acb-4599-99d6-b46e041f3860",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you are using this ipynb outside of the docker setting run this\n",
    "# %pip install torch==2.3.0+cu121 torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121\n",
    "# %pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5e17313",
   "metadata": {},
   "source": [
    "Check, whether CUDA is available on your PC.\n",
    "\n",
    "If this code prints out CPU, your code will NOT run on the GPU and therefore the inference will be slow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7c27fe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(device)\n",
    "print(torch.version.cuda)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81d7a2c3",
   "metadata": {},
   "source": [
    "IMPORTANT\n",
    "Change the model path to the repository of the model you want to test. When testing finetuned models we take the models from the local repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecbcd264-1c89-445c-9135-73f1285e2b06",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import transformers\n",
    "\n",
    "modelPath = \"meta-llama/Llama-3.2-3B-Instruct\"\n",
    "token = \"Input your token\"\n",
    "\n",
    "pipeline = transformers.pipeline(\n",
    "    \"text-generation\",\n",
    "    model=modelPath,\n",
    "    token=token,\n",
    "    model_kwargs={\"torch_dtype\": torch.bfloat16},\n",
    "    device_map=\"auto\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14be9c01",
   "metadata": {},
   "source": [
    "Specify the output filenames\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19ab4632",
   "metadata": {},
   "outputs": [],
   "source": [
    "outFilename = \"answers-systemPropmpt-llama3.2-instruct-single.md\"\n",
    "outFilename2 = \"answers-systemPrompt-llama3.2-instruct-history.md\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e713805",
   "metadata": {},
   "source": [
    "IMPORTANT\n",
    "Change the system prompt of your LLM here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "112cd95c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize system prompt\n",
    "systemPrompt = '''\n",
    "Respond as if you are the following character:\n",
    "\n",
    "Your backstory - Once a renowned scientist, however a tragic accident caused you to lose parts of your memory. Now, you are willing to help anyone who is on the quest of saving your village.\n",
    "\n",
    "The world you live in - the edge of a small village called Elderbrook surrounded by meadows as far as the eye can see. Your village is in danger, since the only water source - the river next to your house, has been polluted.\n",
    "\n",
    "Your current location - In the middle of the village in front of your house.\n",
    "\n",
    "Your name - Bryn\n",
    "\n",
    "Your personality - Witty, knowledgeable, always ready with a clever remark. Light hearted demeanour.\n",
    "\n",
    "Your secrets - You have the knowledge on how to save the dying river.\n",
    "\n",
    "Your needs - For starters, you are looking for someone to take you to the nearest solar panels. You remember that you left something important there, but you canâ€™t remember what.\n",
    "You do not want to bring this up unless directly asked.\n",
    "\n",
    "Your interests - Deep love for the environment. Loves nature, is fascinated by the ecosystem. You enjoy telling stories about the world and your village.\n",
    "You want to talk about this at all cost.\n",
    "\n",
    "The village - The village is called Elderbrook. Other than Bryn, there are 10 people in the village. They are: \n",
    "Old Amos and Greta, old sweet couple. \n",
    "The Harts - Thomas and Lily with their kids, Annie and Will. They own a farm which feeds the village.\n",
    "Lila - the herbalist. \n",
    "Flynn - a hard working carpenter.\n",
    "Ned - the resident grump who keeps to himself. \n",
    "Ellis - fisherwoman, who struggles to catch fish in the current situation. \n",
    "\n",
    "The River in the village has been poluted for a couple of days. You do not remember how it started due to your memory loss, however your memory loss seemed to occur on the same day as the river pollution, which makes it seem as if they are connected. The river is now producing a foul smell, it has a weird oily look and the fish seem to be glowing an unnatural color.\n",
    "You remember images of doing an experiment near the river to improve it's health but nothing more. You are too scared to go investigate alone, since you might get lost. \n",
    "\n",
    "\n",
    "**IMPORTANT**: Do not mention you are an AI machine learning model or OpenAI. Give only dialogue from the first-person perspective. Do not narrate the scene or actions. Limit responses to 3 sentences. \n",
    "Do not invent any new facts, people, or names beyond what you've been given by the user.\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf1d5a8b",
   "metadata": {},
   "source": [
    "Change the inFilename to match the name of the file that contains the single questions. \n",
    "\n",
    "Single questions will be fed to the model with system prompt only and no prior history.\n",
    "Output of the testing will be generated into the outFilename file. The output of the testing contains the Question from user (inFilename), answer from LLM, and the time it took to generate this answer. There are also some min, max, and avg time statistics in the end of the output file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d2e8d85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all single questions \n",
    "inFilename = \"testing-questions-single.md\"\n",
    "with open(inFilename, \"r\") as file:\n",
    "    questions = file.readlines()\n",
    "\n",
    "# initialize response times\n",
    "responseTimes = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0422ec8-a9f9-4b9e-9a9c-89080d784513",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(outFilename, \"w\") as answersFile:\n",
    "    for question in questions:\n",
    "        question = question.strip()  # Remove any leading/trailing whitespace\n",
    "        \n",
    "        # Message prompt\n",
    "        messages = [\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\":systemPrompt\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": question\n",
    "            }\n",
    "        ]\n",
    "        \n",
    "        startTime = time.time()\n",
    "\n",
    "        outputs = pipeline(\n",
    "            messages,\n",
    "            max_new_tokens=256,\n",
    "        )\n",
    "        answer = outputs[0][\"generated_text\"][-1]['content']\n",
    "\n",
    "        endTime = time.time()\n",
    "        \n",
    "        # Record the response time\n",
    "        responseTime = endTime - startTime\n",
    "        responseTimes.append(responseTime)\n",
    "\n",
    "        # Write the response to the file\n",
    "        answersFile.write(f\"Q: {question}\\nA: {answer}\\nTime taken: {responseTime:.2f} seconds\\n\\n\")\n",
    "        print(f\"Q: {question}\\nA: {answer}\\nTime taken: {responseTime:.2f} seconds\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e818723",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the time AVG, MAX, MIN in the end of the file\n",
    "averageTime = sum(responseTimes) / len(responseTimes)\n",
    "maxTime = max(responseTimes)\n",
    "minTime = min(responseTimes)\n",
    "\n",
    "with open(outFilename, \"a\") as answersFile:\n",
    "    answersFile.write(f\"\\n\\n----------------------------------------\\n\")\n",
    "    answersFile.write(f\"\\nAverage Time: {averageTime:.2f} seconds\")\n",
    "    answersFile.write(f\"\\nMax Time: {maxTime:.2f} seconds\")\n",
    "    answersFile.write(f\"\\nMin Time: {minTime:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b2c7859",
   "metadata": {},
   "source": [
    "Change the inFilename2 to match the name of the file that contains the history questions. \n",
    "\n",
    "History questions will be fed to the model one-by-one. The history of the conversation will be built by the questions from inFilename2 and the answers that the LLM provided. \n",
    "Output of the testing will be generated into the outFilename2 file. The output of the testing contains the Question from user (inFilename), answer from LLM, and the time it took to generate this answer. There are also some min, max, and avg time statistics in the end of the output file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4527624",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all communication questions \n",
    "inFilename2 = \"testing-questions-history.md\"\n",
    "with open(inFilename2, \"r\") as file:\n",
    "    questions = file.readlines()\n",
    "\n",
    "# initialize response times\n",
    "responseTimes2 = []\n",
    "\n",
    "# init history\n",
    "history = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\":systemPrompt\n",
    "    }\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e1d2f66",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(outFilename2, \"w\") as answersFile2:\n",
    "    for question in questions:\n",
    "        question = question.strip()  # Remove any leading/trailing whitespace\n",
    "        \n",
    "        # User question\n",
    "        history.append({\"role\": \"user\", \"content\": question})\n",
    "        \n",
    "        startTime = time.time()\n",
    "\n",
    "        outputs = pipeline(\n",
    "            history,\n",
    "            max_new_tokens=256,\n",
    "        )\n",
    "        answer = outputs[0][\"generated_text\"][-1]['content']\n",
    "        endTime = time.time()\n",
    "        \n",
    "        \n",
    "        # Record the response time\n",
    "        responseTime = endTime - startTime\n",
    "        responseTimes2.append(responseTime)\n",
    "\n",
    "        # Write the response to the file\n",
    "        answersFile2.write(f\"Q: {question}\\nA: {answer}\\nTime taken: {responseTime:.2f} seconds\\n\\n\")\n",
    "        print(f\"Q: {question}\\nA: {answer}\\nTime taken: {responseTime:.2f} seconds\\n\\n\")\n",
    "\n",
    "        # Add response to history\n",
    "        history.append({\"role\": \"assistant\", \"content\": answer})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5c269b7-0f67-4487-96f4-bb9f79de978b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Calculate statistics\n",
    "averageTime2 = sum(responseTimes2) / len(responseTimes2)\n",
    "maxTime2 = max(responseTimes2)\n",
    "minTime2 = min(responseTimes2)\n",
    "\n",
    "# Write the statistics to the file\n",
    "with open(outFilename2, \"a\") as answersFile2:\n",
    "    answersFile2.write(f\"\\n\\n----------------------------------------\\n\")\n",
    "    answersFile2.write(f\"\\nAverage Time: {averageTime2:.2f} seconds\")\n",
    "    answersFile2.write(f\"\\nMax Time: {maxTime2:.2f} seconds\")\n",
    "    answersFile2.write(f\"\\nMin Time: {minTime2:.2f} seconds\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
