{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "db03ac02-d820-451f-95f7-00250392bf21",
   "metadata": {},
   "source": [
    "Finetuning is done using the Unsloth library, and the following guide: https://huggingface.co/blog/mlabonne/sft-llama3\n",
    "\n",
    "Also available for Mistral, Gemma,... \n",
    "https://huggingface.co/unsloth/llama-3-8b-Instruct\n",
    "\n",
    "\n",
    "Note:\n",
    "Needs to use Python 3.11\n",
    "Can be ran through docker. All requirements will be installed and the environment will be prepared. Open ReadMe to learn more.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4d67cfe3-3a0c-46dc-80ce-a75c6ff56958",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you are using this ipynb outside of the docker setting run this\n",
    "# %pip install torch==2.3.0+cu121 torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121\n",
    "# %pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25c823ef-97cc-4898-805e-ac64ae878be7",
   "metadata": {},
   "source": [
    "The following code should return True. If it doesn't the finetuning will be unsuccessfull as it cannot run on GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0e10b27f-9bd9-4ad2-8516-d5e85c7e8141",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "12.1\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.version.cuda)\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "824b36b0-b846-49ea-aa6a-bd3abff65fd7",
   "metadata": {},
   "source": [
    "Setup all the names\n",
    "Select the name of the model, the path to the datasets and the outputs. \n",
    "\n",
    "The model is downloaded from the unsloth repository on HF. Select the -bnb-4bit model is you want to use QLoRA finetuning. Select the regular model for LoRA.\n",
    "\n",
    "Set the load4 bit flag to True, if you want to use QLoRA finetuning, False if you want to use LoRA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "406d255c-333d-4d76-a64a-d14ff91410e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "modelName = \"unsloth/Llama-3.2-3B-Instruct\"                   #Instruct LoRA\n",
    "#modelName = \"unsloth/Llama-3.2-3B-Instruct-bnb-4bit\"           #Instruct QLoRA\n",
    "\n",
    "inFilename = \"../TrainingDatasets/reworked-training-questions-30.json\"\n",
    "modelOutputDir = \"Models/llama3.2-instruct-lora-30\"\n",
    "modelLocalOutput = \"Models/Output/Llama3.2-instruct-lora-30\"\n",
    "outFilename = \"llama3.2-instruct-lora-30-stats.md\"\n",
    "\n",
    "load4bit = False   # LoRA\n",
    "#load4bit = True   # QLoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3a8d3ab8-1216-4ec9-80f3-e4f04af32fad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n",
      "==((====))==  Unsloth 2024.12.2: Fast Llama patching. Transformers:4.46.1.\n",
      "   \\\\   /|    GPU: NVIDIA GeForce RTX 4090. Max memory: 23.643 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.3.0+cu121. CUDA: 8.9. CUDA Toolkit: 12.1. Triton: 2.3.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.26.post1. FA2 = False]\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth 2024.12.2 patched 28 layers with 28 QKV layers, 28 O layers and 28 MLP layers.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from trl import SFTTrainer\n",
    "from datasets import load_dataset\n",
    "from transformers import TrainingArguments, TextStreamer\n",
    "from unsloth.chat_templates import get_chat_template\n",
    "from unsloth import FastLanguageModel, is_bfloat16_supported\n",
    "\n",
    "max_seq_length = 2048\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=modelName,\n",
    "    max_seq_length=max_seq_length,\n",
    "    load_in_4bit=load4bit,\n",
    "    dtype=None,\n",
    ")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "#model.to(device)  # Only if using Lora, comment out otherwise\n",
    "\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r=16,\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0,\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"up_proj\", \"down_proj\", \"o_proj\", \"gate_proj\"], \n",
    "    use_rslora=True,\n",
    "    use_gradient_checkpointing=\"unsloth\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25e1491f-110f-4318-8b36-2540d5f845ef",
   "metadata": {},
   "source": [
    "Load the training file and modify it to fit the template \n",
    "(Here you can choose between the training files containing different amount of questions)\n",
    "\n",
    "<|im_start|>user\n",
    "Remove the spaces from the following sentence: It prevents users to suspect that there are some hidden products installed on theirs device.\n",
    "<|im_end|>\n",
    "<|im_start|>assistant\n",
    "Itpreventsuserstosuspectthattherearesomehiddenproductsinstalledontheirsdevice.<|im_end|>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2e953ea8-2ecc-4e63-85c7-fc71dd2cecfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Will map <|im_end|> to EOS = <|eot_id|>.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = get_chat_template(\n",
    "    tokenizer,\n",
    "    mapping={\"role\": \"role\", \"content\": \"value\", \"user\": \"human\", \"assistant\": \"chatbot\", \"system\" : \"system\"},\n",
    "    chat_template=\"chatml\",\n",
    ")\n",
    "\n",
    "# Transform the quesitons to the chatml template\n",
    "def apply_template(examples):\n",
    "    messages = examples[\"conversations\"]\n",
    "    # Apply the chat template to each message in the conversation\n",
    "    text = [tokenizer.apply_chat_template(\n",
    "                [{\"role\": msg[\"role\"], \"content\": msg[\"value\"]} for msg in message], \n",
    "                tokenize=False, \n",
    "                add_generation_prompt=False) \n",
    "            for message in messages]\n",
    "    return {\"text\": text}\n",
    "\n",
    "\n",
    "dataset = load_dataset('json', data_files=inFilename, split=\"train\")\n",
    "dataset = dataset.shuffle()\n",
    "dataset = dataset.map(apply_template, batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1b66899-32af-464c-aaa3-b080d4266471",
   "metadata": {},
   "source": [
    "https://medium.com/@gobishangar11/llama-2-a-detailed-guide-to-fine-tuning-the-large-language-model-8968f77bcd15\n",
    "Guide for finetuning parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4d3dc76d-c9f0-4b4c-b378-dbb163f0b20c",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer=SFTTrainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=dataset,\n",
    "    dataset_text_field=\"text\",\n",
    "    max_seq_length=max_seq_length,\n",
    "    dataset_num_proc=2,\n",
    "    packing=True,\n",
    "    \n",
    "    args=TrainingArguments(\n",
    "        output_dir = modelOutputDir,\n",
    "        num_train_epochs=3,\n",
    "        per_device_train_batch_size=4,\n",
    "        gradient_accumulation_steps=1,\n",
    "        optim=\"paged_adamw_32bit\",\n",
    "        logging_steps = 1,\n",
    "        learning_rate=3e-4,\n",
    "        weight_decay=0.001,\n",
    "        fp16=not is_bfloat16_supported(),\n",
    "        bf16=is_bfloat16_supported(),\n",
    "        max_grad_norm=0.3,\n",
    "        max_steps=-1,\n",
    "        warmup_ratio=0.03,\n",
    "        group_by_length=True,\n",
    "        lr_scheduler_type=\"constant\",\n",
    "        report_to=\"tensorboard\"\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08482855-02bc-4ef2-97f2-5a30b8b10c53",
   "metadata": {},
   "source": [
    "Save current memory and time stats into a file + print them out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cdea4336-48ae-4675-948f-d81d98ee04fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU = NVIDIA GeForce RTX 4090. Max memory = 23.643 GB.\n",
      "6.539 GB of memory reserved.\n"
     ]
    }
   ],
   "source": [
    "gpuStats = torch.cuda.get_device_properties(0)\n",
    "startGpuMemory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "maxMemory = round(gpuStats.total_memory / 1024 / 1024 / 1024, 3)\n",
    "print(f\"GPU = {gpuStats.name}. Max memory = {maxMemory} GB.\")\n",
    "print(f\"{startGpuMemory} GB of memory reserved.\")\n",
    "\n",
    "with open(outFilename, \"w\") as statsFile:\n",
    "    statsFile.write(f\"GPU = {gpuStats.name}. Max memory = {maxMemory} GB.\")\n",
    "    statsFile.write(f\"{startGpuMemory} GB of memory reserved.\")\n",
    "    statsFile.flush()     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "786a69d5-e0cb-497f-a5f7-f0334aa82b80",
   "metadata": {},
   "source": [
    "Finetune the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "58e64185-2015-4479-8878-5fc712873d60",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
      "   \\\\   /|    Num examples = 5 | Num Epochs = 3\n",
      "O^O/ \\_/ \\    Batch size per device = 4 | Gradient Accumulation steps = 1\n",
      "\\        /    Total batch size = 4 | Total steps = 6\n",
      " \"-____-\"     Number of trainable parameters = 24,313,856\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='6' max='6' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [6/6 00:04, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.997700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.788100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.676300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.612400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.481600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.365000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer_stats = trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "606e30cb-bfe6-41de-ba9a-13e50756ff59",
   "metadata": {},
   "source": [
    "Save final memory and time stats into a file + print them out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dc6ad5e4-495f-446a-959d-6ab57ca07ecf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.6111 seconds used for training.\n",
      "0.14 minutes used for training.\n",
      "Peak reserved memory = 12.885 GB.\n",
      "Peak reserved memory for training = 6.346 GB.\n",
      "Peak reserved memory % of max memory = 54.498 %.\n",
      "Peak reserved memory for training % of max memory = 26.841 %.\n"
     ]
    }
   ],
   "source": [
    "usedMemory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "usedMemoryForLora = round(usedMemory - startGpuMemory, 3)\n",
    "usedPercentage = round(usedMemory         /maxMemory*100, 3)\n",
    "loraPercentage = round(usedMemoryForLora/maxMemory*100, 3)\n",
    "\n",
    "print(f\"{trainer_stats.metrics['train_runtime']} seconds used for training.\")\n",
    "print(f\"{round(trainer_stats.metrics['train_runtime']/60, 2)} minutes used for training.\")\n",
    "print(f\"Peak reserved memory = {usedMemory} GB.\")\n",
    "print(f\"Peak reserved memory for training = {usedMemoryForLora} GB.\")\n",
    "print(f\"Peak reserved memory % of max memory = {usedPercentage} %.\")\n",
    "print(f\"Peak reserved memory for training % of max memory = {loraPercentage} %.\")\n",
    "\n",
    "\n",
    "with open(outFilename, \"w\") as statsFile:\n",
    "    statsFile.write(f\"{trainer_stats.metrics['train_runtime']} seconds used for training.\")\n",
    "    statsFile.write(f\"{round(trainer_stats.metrics['train_runtime']/60, 2)} minutes used for training.\")\n",
    "    statsFile.write(f\"Peak reserved memory = {usedMemory} GB.\")\n",
    "    statsFile.write(f\"Peak reserved memory for training = {usedMemoryForLora} GB.\")\n",
    "    statsFile.write(f\"Peak reserved memory % of max memory = {usedPercentage} %.\")\n",
    "    statsFile.write(f\"Peak reserved memory for training % of max memory = {loraPercentage} %.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3507cd00-4c1b-4edb-8b14-9d3516a5b61f",
   "metadata": {},
   "source": [
    "Save the model and tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fcc775e5-78b5-41fb-a9fe-30783023fad0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Models/Output/Llama3.2-instruct-lora-30/tokenizer_config.json',\n",
       " 'Models/Output/Llama3.2-instruct-lora-30/special_tokens_map.json',\n",
       " 'Models/Output/Llama3.2-instruct-lora-30/tokenizer.json')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.save_pretrained(modelLocalOutput)\n",
    "tokenizer.save_pretrained(modelLocalOutput)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
