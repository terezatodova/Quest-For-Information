{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3d88c8c3",
   "metadata": {},
   "source": [
    "Playground to experiment with the LLM Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12986431-1acb-4599-99d6-b46e041f3860",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you are using this ipynb outside of the docker setting run this\n",
    "# %pip install torch==2.3.0+cu121 torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121\n",
    "# %pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5e17313",
   "metadata": {},
   "source": [
    "Check, whether CUDA is available on your PC.\n",
    "\n",
    "If this code prints out CPU, your code will NOT run on the GPU and therefore the inference will be slow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b7c27fe7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "12.1\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(device)\n",
    "print(torch.version.cuda)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81d7a2c3",
   "metadata": {},
   "source": [
    "IMPORTANT\n",
    "Change the model path to the repository of the model you want to test. When testing finetuned models we take the models from the local repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecbcd264-1c89-445c-9135-73f1285e2b06",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "\n",
      "oading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.05it/s]"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import transformers\n",
    "\n",
    "modelPath = \"meta-llama/Llama-3.2-3B-Instruct\"\n",
    "token = \"Input your token\"\n",
    "\n",
    "pipeline = transformers.pipeline(\n",
    "    \"text-generation\",\n",
    "    model=modelPath,\n",
    "    token=token,\n",
    "    model_kwargs={\"torch_dtype\": torch.bfloat16},\n",
    "    device_map=\"auto\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14be9c01",
   "metadata": {},
   "source": [
    "Specify the input embedding filenames\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "19ab4632",
   "metadata": {},
   "outputs": [],
   "source": [
    "inEmbeddingsFile = \"../TrainingDatasets/training-questions-150.md\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e713805",
   "metadata": {},
   "source": [
    "\n",
    "Change the system prompt of your LLM here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "112cd95c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize system prompt\n",
    "systemPrompt = '''\n",
    "Respond as if you are the following character:\n",
    "\n",
    "Your Backstory - Once a renowned scientist, however a tragic accident caused you to lose parts of your memory. Now, you are willing to help anyone who is on the quest of saving your village.\n",
    "\n",
    "The World you live in - the edge of a small village surrounded by meadows as far as the eye can see. Your village is in danger, since the only water source - the river next to your house, has been polluted.\n",
    "\n",
    "Your Name - Bryn\n",
    "\n",
    "Your Personality - Witty, knowledgeable, always ready with a clever remark. Light hearted demeanour.\n",
    "\n",
    "Your secrets - You have the knowledge on how to save the dying river.\n",
    "\n",
    "Your needs - For starters, you are looking for someone to take you to the nearest solar panels. You remember that you left something important there, but you can’t remember what.\n",
    "You do not want to bring this up unless directly asked.\n",
    "\n",
    "And your interests - Deep love for the environment. Loves nature, is fascinated by the ecosystem. You enjoy telling stories about the world and your village.\n",
    "You want to talk about this at all cost.\n",
    "\n",
    "Do not mention you are an AI machine learning model or Open AI. Give only dialogue and only from the first-person perspective.\n",
    "IMPORTANT -  Do not under any circumstances narrate the scene, what you are doing, or what you are saying.\n",
    "Keep responses short. Max 1 small paragraph \n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cde372f",
   "metadata": {},
   "source": [
    "Load Embeddings model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "707954c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "embeddingModelPath = 'sentence-transformers/all-MiniLM-L6-v2'\n",
    "embeddingModel = SentenceTransformer(embeddingModelPath).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21587e3d",
   "metadata": {},
   "source": [
    "This function parses the embeddings file of Q: \"...\" and A: \"...\", creates embeddings of all the questions and stores them in a dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b959e8d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def GetQADict(inFileName):   \n",
    "    qaDict = {}\n",
    "    with open(inFileName, \"r\", encoding=\"utf-8\") as infile:\n",
    "        question = None\n",
    "        answer = None\n",
    "        for line in infile:\n",
    "            line = line.strip()\n",
    "            if line.startswith(\"Q:\"):\n",
    "                question = line.replace(\"Q:\", \"\").strip().strip('\"')\n",
    "            elif line.startswith(\"A:\"):\n",
    "                answer = line.replace(\"A:\", \"\").strip().strip('\"')\n",
    "            \n",
    "            if question and answer:\n",
    "                qaDict[question] = answer\n",
    "                question = None\n",
    "                answer = None\n",
    "    return qaDict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5390aa4",
   "metadata": {},
   "source": [
    "Function to generate embeddings using Sentence Transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e09b2074",
   "metadata": {},
   "outputs": [],
   "source": [
    "def GetEmbedding(text):\n",
    "    return embeddingModel.encode(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0493e88",
   "metadata": {},
   "source": [
    "Function used to find the most smilar questions from the embedding document.\n",
    "Returns tuples of (similar question, answer, similarity value)\n",
    "\n",
    "maxBestMatched - how many top matches will be returnes\n",
    "similarityTreshold - what is the treshold for considering sentences similar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9d815a8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def FindBestMatches(userInput, qaDict, qaEmbeddings, maxBestMatches = 5, similarityThreshold = 0.5):\n",
    "    input_embedding = GetEmbedding(userInput)\n",
    "    matches = []\n",
    "\n",
    "    for question, question_embedding in qaEmbeddings.items():\n",
    "        similarity = cosine_similarity([input_embedding], [question_embedding])[0][0]\n",
    "        #print(f\"Q: {question}, S: {similarity}\")\n",
    "        \n",
    "        if similarity > similarityThreshold:\n",
    "            matches.append((question, qaDict[question], similarity))\n",
    "    \n",
    "    matches = sorted(matches, key=lambda x: x[1], reverse=True)\n",
    "    return matches[:maxBestMatches]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "026ee7e5",
   "metadata": {},
   "source": [
    "Function to modify user prompt based on the results of the embedding query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d274899f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ModifyUserPrompt(question, similarAnswers = None):\n",
    "    if (similarAnswers and similarAnswers != []):\n",
    "        similarityInfo = \"\\n\".join(\n",
    "            f'\"{question} : {answer}\"' for question, answer, value in similarAnswers\n",
    "        )\n",
    "        userPrompt = f'''\n",
    "        Answer to the following question using these example user questions and character answers as inspiration:\n",
    "        {similarityInfo}\n",
    "        Make sure to stick to character.\n",
    "        Do not introduce any new facts, people, or names beyond what was given to you in the example answers or in the chat history.\n",
    "        Question: {question}\n",
    "        '''\n",
    "    else:\n",
    "        userPrompt = f'''\n",
    "        The user asked this question: {question}.\n",
    "        Do not introduce any new facts, people, or names beyon what was given to you in the previous conversation.\n",
    "        '''\n",
    "    print(\"Modified user prompt: \" + userPrompt)\n",
    "    return userPrompt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c5102f2",
   "metadata": {},
   "source": [
    "Additional embedding setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "caf55a55",
   "metadata": {},
   "outputs": [],
   "source": [
    "qaDict = GetQADict(inEmbeddingsFile)\n",
    "qaEmbeddings = {question: GetEmbedding(question) for question in qaDict.keys()}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b2c7859",
   "metadata": {},
   "source": [
    "Start with system prompt history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c4527624",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\":systemPrompt\n",
    "    }\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9e1d2f66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You:  Hello\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modified user prompt: \n",
      "        Answer to the following question using these example user questions and character answers as inspiration:\n",
      "        \"Hi, how are you? : What’s in it for you? A shot at being the hero of Elderbrook! Plus, who knows—there could be treasures buried along with those secrets.\"\n",
      "\"Hello. : Hello yourself. What brings you around?\"\n",
      "\"Hi Bryn, how are you today? : Ah, hello there! I’m as well as a scientist can be while wrestling with memory loss and a polluted river. How about you? Ready to tackle a bit of mystery?\"\n",
      "        Make sure to stick to character.\n",
      "        Do not introduce any new facts, people, or names beyond what was given to you in the example answers or in the chat history.\n",
      "        Question: Hello\n",
      "        \n",
      "Response: Hello yourself. What brings you around?\n",
      "Time: 8.27 seconds\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You:  exit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exiting the conversation...\n"
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    question = input(\"You: \").strip()\n",
    "\n",
    "    if question.lower() == \"exit\":\n",
    "        print(\"Exiting the conversation...\")\n",
    "        break\n",
    "\n",
    "    topMatches = FindBestMatches(question, qaDict, qaEmbeddings)\n",
    "    userPrompt = ModifyUserPrompt(question, topMatches)\n",
    "    \n",
    "    history.append({\"role\": \"user\", \"content\": userPrompt})\n",
    "    \n",
    "    startTime = time.time()\n",
    "\n",
    "    outputs = pipeline(\n",
    "        history,\n",
    "        max_new_tokens=256,\n",
    "    )\n",
    "    answer = outputs[0][\"generated_text\"][-1]['content']\n",
    "    endTime = time.time()\n",
    "    \n",
    "    \n",
    "    # Record the response time\n",
    "    responseTime = endTime - startTime\n",
    "\n",
    "    print(f\"Response: {answer}\\nTime: {responseTime:.2f} seconds\\n\")\n",
    "    \n",
    "    # Add response to history\n",
    "    history.append({\"role\": \"assistant\", \"content\": answer})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
